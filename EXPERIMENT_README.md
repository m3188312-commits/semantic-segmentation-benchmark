# Pseudo-Labeling Experiments

This system runs comprehensive experiments to evaluate the effectiveness of pseudo-labeling with model agreement for semantic segmentation.

## Overview

The experiments test **18 different configurations** plus a baseline:

- **K values**: 10, 50, 100 (number of pseudo-images to include)
- **N values**: 2, 3, 4 (agreement level - how many models must agree)
- **Variants**: 
  - `no-remove`: Use original unlabeled images + consensus masks
  - `remove-unknown`: Use gray-painted images + consensus masks
- **Baseline**: Train only on original labeled data

**Total**: 3 × 3 × 2 = 18 experiments + 1 baseline = 19 runs

## Quick Start

### 1. Run Experiments

```bash
# Full run (50 epochs per model)
python run_experiments.py

# Quick test (5 epochs per model)
python run_experiments.py --test

# Skip baseline training
python run_experiments.py --skip-baseline
```

### 2. Analyze Results

```bash
# Generate analysis and plots
python analyze_results.py

# Just show summary (no plots)
python analyze_results.py --no-plots
```

## File Structure

```
semantic-segmentation-benchmark/
├── dataset/
│   ├── train/image/          # Original labeled images
│   ├── train/mask/           # Original labeled masks  
│   └── test/                 # Test set for evaluation
├── data/
│   ├── unlabeled/            # Original unlabeled images
│   ├── pseudo_labels/        # Consensus masks by agreement level
│   │   ├── agreement_2/
│   │   ├── agreement_3/
│   │   └── agreement_4/
│   └── pseudo_images/        # Gray-painted images by agreement level
│       ├── agreement_2/
│       ├── agreement_3/
│       └── agreement_4/
├── checkpoints/              # Trained model weights (created during experiments)
├── plots/                    # Analysis plots (created by analyze_results.py)
└── experiment_results.csv    # Results table (created during experiments)
```

## Configuration

### Main Parameters (in `pseudo_labeling_experiments.py`)

```python
# Training parameters
NUM_EPOCHS = 50          # Training epochs per model
BATCH_SIZE = 4           # Batch size
LEARNING_RATE = 1e-4     # Learning rate
IMAGE_SIZE = (512, 512)  # Input image size
NUM_CLASSES = 8          # Number of semantic classes
```

### Directory Paths

All paths are configurable at the top of `pseudo_labeling_experiments.py`:

```python
LABELED_IMAGES_DIR = Path("dataset/train/image")
LABELED_MASKS_DIR = Path("dataset/train/mask")
TEST_IMAGES_DIR = Path("dataset/test/image")
TEST_MASKS_DIR = Path("dataset/test/mask")
# ... etc
```

## How It Works

### 1. Data Assembly (`build_subset()`)

For each experiment configuration:

1. **Always include** all original labeled images + masks from `dataset/train/`
2. **Select top K pseudo-images** based on mask coverage (% non-unknown pixels)
3. **Choose image source** based on variant:
   - `no-remove`: Use original images from `data/unlabeled/`
   - `remove-unknown`: Use gray-painted images from `data/pseudo_images/agreement_N/`
4. **Use consensus masks** from `data/pseudo_labels/agreement_N/`

### 2. Training (`train_model()`)

- Uses DeepLabV3 with ResNet-50 backbone
- Pre-trained on ImageNet, fine-tuned for 8-class segmentation
- Saves model checkpoints as `deeplab_K{K}_N{N}_{variant}.pth`

### 3. Evaluation (`evaluate_model()`)

- Evaluates on `dataset/test/` (same test set for all experiments)
- Computes macro-averaged precision, recall, F1-score
- Handles class imbalance with `zero_division=0`

### 4. Pseudo-Image Selection

Images are selected by **mask coverage** (percentage of non-unknown pixels):

```python
coverage = (total_pixels - unknown_pixels) / total_pixels
```

- Higher coverage = more confident pseudo-labels
- Ties broken by filename (alphabetical sort)

## Output Files

### `experiment_results.csv`

Contains results for all experiments:

```csv
run_id,K,N,variant,precision,recall,f1
baseline,0,N/A,labeled_only,0.7234,0.6891,0.7058
deeplab_K10_N2_no-remove,10,2,no-remove,0.7456,0.7123,0.7286
deeplab_K10_N2_remove-unknown,10,2,remove-unknown,0.7389,0.7067,0.7225
...
```

### Model Checkpoints

Saved in `checkpoints/` directory:
- `baseline.pth`
- `deeplab_K10_N2_no-remove.pth`
- `deeplab_K10_N2_remove-unknown.pth`
- ... (19 total)

### Analysis Plots

Generated by `analyze_results.py` in `plots/` directory:
- `f1_heatmap.png`: F1 scores by K and N for each variant
- `variant_comparison.png`: Bar chart comparing variants
- `k_effect.png`: Line plots showing effect of K
- `baseline_comparison.png`: All experiments vs baseline

## Expected Runtime

**Full experiments (50 epochs each)**:
- ~19 models × 50 epochs × ~5 min/epoch = ~79 hours on GPU
- Consider using `--test` mode first (5 epochs each, ~8 hours)

**Quick test (5 epochs each)**:
- ~19 models × 5 epochs × ~5 min/epoch = ~8 hours on GPU

## Customization

### Different K values

```python
K_values = [5, 25, 75]  # Modify in main()
```

### Different agreement levels

```python
N_values = [2, 3]  # Skip N=4 if too restrictive
```

### Different model architectures

Replace `build_deeplab_model()` with your preferred architecture.

### Different evaluation metrics

Modify `evaluate_model()` to compute IoU, pixel accuracy, etc.

## Troubleshooting

### Common Issues

1. **CUDA out of memory**: Reduce `BATCH_SIZE` in the script
2. **Missing test data**: Ensure `dataset/test/` contains image-mask pairs
3. **No pseudo-images found**: Check that `data/pseudo_labels/` contains masks
4. **File path errors**: Verify all directory paths in the configuration section

### Debug Mode

Add `--test` flag to run with fewer epochs for debugging:

```bash
python run_experiments.py --test
```

### Resume Interrupted Experiments

The system doesn't support resuming yet. To skip completed experiments, manually modify the loops in `main()`.

## Results Interpretation

### Key Metrics

- **Precision**: How many predicted pixels were correct
- **Recall**: How many ground truth pixels were found  
- **F1-score**: Harmonic mean of precision and recall

### Expected Findings

- **More pseudo-images (higher K)** may improve performance up to a point
- **Higher agreement (higher N)** gives more confident but fewer pseudo-labels
- **Variant comparison** shows whether gray-painting unknown regions helps or hurts
- **Best configuration** balances quantity and quality of pseudo-labels

### Analysis Questions

1. Does pseudo-labeling improve over baseline?
2. What's the optimal number of pseudo-images (K)?
3. What's the optimal agreement level (N)?
4. Is it better to gray-paint unknown regions or use original images?
5. How do the benefits vary by semantic class?

## Citation

If you use this experimental framework, please cite your semantic segmentation work and mention the pseudo-labeling approach with model agreement.
