# semantic-segmentation-benchmark

> Benchmark and compare DeepLabV3, YOLOâ€‘Seg, Uâ€‘Net and Random Forest models for semantic segmentation on satellite imagery.

---

## ğŸš€ Overview

**semantic-segmentation-benchmark** is an end-to-end suite designed to:

* **Train**: Fit multiple segmentation models (DeepLabV3, YOLOâ€‘Seg, Uâ€‘Net, Random Forest) on curated datasets.
* **Evaluate**: Quantify per-class and aggregate metrics (Precision, Recall, F1, pixel accuracy) on validation splits or custom image subsets.
* **Infer**: Generate segmentation masks for new images via a unified CLI interface.
* **Visualize**: Produce professional visual artifactsâ€”heatmaps, grouped bar charts, radar plotsâ€”for transparent comparative analysis.


---

## ğŸ“ Repository Structure

```
semantic-segmentation-benchmark/
â”œâ”€â”€ README.md                     # Project documentation
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ .gitignore                    # Excluded files
â”œâ”€â”€ data/                         # Raw and processed datasets
â”‚   â”œâ”€â”€ raw/
â”‚   â””â”€â”€ processed/
â”œâ”€â”€ models/                       # Model training and checkpoints
â”‚   â”œâ”€â”€ deeplab/
â”‚   â”œâ”€â”€ yolo/
â”‚   â”œâ”€â”€ unet/
â”‚   â””â”€â”€ random_forest/
â”œâ”€â”€ scripts/                      # Orchestrator scripts
â”‚   â”œâ”€â”€ train_all.py              # Train all models sequentially
â”‚   â”œâ”€â”€ eval_all.py               # Batch evaluation script
â”‚   â””â”€â”€ infer.py                  # Single-image inference wrapper
â”œâ”€â”€ notebooks/                    # Exploratory analysis and dashboards
â”‚   â”œâ”€â”€ 01_data_exploration.ipynb
â”‚   â”œâ”€â”€ 02_training_comparison.ipynb
â”‚   â””â”€â”€ 03_visualization_dashboard.ipynb
â””â”€â”€ output/                       # Generated artifacts
    â”œâ”€â”€ checkpoints/              # Best/final model weights
    â”œâ”€â”€ metrics/                  # CSVs of computed metrics
    â””â”€â”€ figures/                  # Visualization outputs
```

---

## âš™ï¸ Setup & Installation

1. **Clone the repository**:

   ```bash
   git clone https://github.com/aggelosntou/semantic-segmentation-benchmark.git
   cd semantic-segmentation-benchmark
   ```

2. **Create & activate** a Python virtual environment:

   ```bash
   python3 -m venv .venv
   source .venv/bin/activate         # macOS/Linux
   .\.venv\Scripts\activate.ps1    # Windows PowerShell
   ```

3. **Install dependencies**:

   ```bash
   pip install --upgrade pip
   pip install -r requirements.txt
   ```

---

## ğŸ¯ Usage

### 1. Train & Evaluate

Run a full training cycle followed by validation:

```bash
python trainingDL.py
```

### 2. Inference Only

Generate a mask for a single image:

```bash
python trainingDL.py \
  --infer \
  --image /path/to/input.jpg \
  --out   ./output/mask.png
```

### 3. Full Validation Only

Compute metrics on the entire validation set:

```bash
python trainingDL.py --eval
```

### 4. Subset Evaluation

Evaluate on a specific list of images:

```bash
python trainingDL.py --eval_images img1.jpg img2.jpg img3.jpg
```

---

## ğŸ“Š Visualization

Leverage the notebooks or scripts to generate:

* **Heatmaps** of per-class F1.
* **Grouped bar charts** comparing models across classes.
* **Radar charts** illustrating holistic model performance profiles.


---

## ğŸ› ï¸ Extensibility

* Add new architectures by placing training logic in `models/<model_name>/` and updating `scripts/train_all.py`.
* Integrate additional metrics (e.g. boundary F1, ROC curves) by extending the evaluation functions in `scripts/eval_all.py`.

---
